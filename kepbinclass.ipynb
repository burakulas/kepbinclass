{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/burakulas/kepbinclass/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbxcfxkPBPZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "KEPBINCLASS----\n",
        "\n",
        "This script predicts the morphological class of the Kepler binary star \n",
        "systems through deep learning algorithm and recognizing their light \n",
        "curve images.\n",
        "\n",
        "Change the path of train and validation data directory values based your \n",
        "directory scheme. \n",
        "\n",
        "The script works on Python 3.x. Some modifications are needed to get it \n",
        "run in Python 2.x\n",
        "\n",
        "The script uses TensorFlow 1.x. Some modifications are needed to get it \n",
        "run in TensorFlow 2.x\n",
        "'''\n",
        "\n",
        "# import necessary modules\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "from google.colab import files\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)\n",
        "\n",
        "# pass deprecated warnings from TF 2.0\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "\n",
        "img_width, img_height = 640, 480 # size of images\n",
        "\n",
        "# train and validation directory\n",
        "train_data_dir = '/path/to/train'\n",
        "validation_data_dir = '/path/to/validation'\n",
        "n_train = 1458 # number of train images\n",
        "n_valid = 294 # number of validation images\n",
        "epoch = 50 # number of epochs\n",
        "batch = 16 # batch size\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    input_shape = (3, img_width, img_height)\n",
        "else:\n",
        "    input_shape = (img_width, img_height, 3)\n",
        "\n",
        "\n",
        "# the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(0.001),\n",
        "                 input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# augmentation configuration\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch,\n",
        "    class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch,\n",
        "    class_mode='categorical')\n",
        "\n",
        "\n",
        "#fitting\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=n_train // batch,\n",
        "    epochs=epoch,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=n_valid // batch, shuffle=False)\n",
        "\n",
        "\n",
        "#saving model\n",
        "\n",
        "model.save_weights('mod_binclass.h5')\n",
        "\n",
        "\n",
        "# plotting and saving the accuracy and loss function\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.savefig('acc.png')\n",
        "plt.show()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Prediction - a specific file\n",
        "\n",
        "img_width, img_height = 640, 480\n",
        "img = image.load_img('/path/to/prediction.png', \n",
        "                     target_size = (img_width, img_height))\n",
        "img = image.img_to_array(img)\n",
        "img = np.expand_dims(img, axis = 0)\n",
        "\n",
        "pred = model.predict_classes(img) # class prediction\n",
        "#pred = model.predict_proba(img) # probability prediction\n",
        "print(\"---Prediction-----\")\n",
        "print(pred)\n",
        "\n",
        "\n",
        "# Prediction - a bunch of images in folder\n",
        "\n",
        "pred_data_dir = '/path/to/prediction'\n",
        "\n",
        "images = []\n",
        "for img in os.listdir(pred_data_dir):\n",
        "    imgn = img\n",
        "    img = os.path.join(pred_data_dir, img)\n",
        "    img = image.load_img(img, target_size=(img_width, img_height))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = img / 255\n",
        "    classp = model.predict_classes(img)\n",
        "    print(imgn,classp) "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
